{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitdanlpcondab70fe6e4720d408985175c9cab14fc3e",
   "display_name": "Python 3.8.5 64-bit ('danlp': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Getting started with DaNLP\n",
    "\n",
    "This tutorial provides you with code for getting started with the DaNLP package for the different tasks we cover. \n",
    "More information can be found in the docs folder for each model/dataset. \n",
    "This tutorial reuses the code snippets from the documentation as minimal examples.\n",
    "\n",
    "Overview: \n",
    "\n",
    "1. Models\n",
    "2. Datasets\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Models\n",
    "\n",
    "    1.1. Word embeddings\n",
    "    1.2. Part-of-speech tagging\n",
    "    1.3. Named entity recognition\n",
    "    1.4. Dependency Parsing & Noun Phrase Chunking\n",
    "    1.5. Sentiment Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1 Word embeddings\n",
    "\n",
    "You can choose between using static or dynamic word embeddings. \n",
    "\n",
    "Below is an example of how to download and load pretrained static word embeddings with gensim or spaCy. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models.embeddings  import load_wv_with_gensim\n",
    "\n",
    "# Load with gensim\n",
    "word_embeddings = load_wv_with_gensim('conll17.da.wv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "word_embeddings.most_similar(positive=['københavn', 'england'], negative=['danmark'], topn=1)\n",
    "word_embeddings.doesnt_match(\"vand sodavand brød vin juice\".split())\n",
    "word_embeddings.similarity('københavn', 'århus')\n",
    "word_embeddings.similarity('københavn', 'esbjerg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models.embeddings  import load_wv_with_spacy\n",
    "# Load with spacy\n",
    "word_embeddings = load_wv_with_spacy('conll17.da.wv')"
   ]
  },
  {
   "source": [
    "Here is an example of how to load the pretrained dynamic flair embeddings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models.embeddings import load_context_embeddings_with_flair\n",
    "\n",
    "\n",
    "# Use the wrapper from DaNLP to download and load embeddings with Flair\n",
    "# You can combine it with on of the static emebdings\n",
    "stacked_embeddings = load_context_embeddings_with_flair(word_embeddings='wiki.da.wv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "# test \n",
    "\n",
    "# Embedd two different sentences\n",
    "sentence1 = Sentence('Han fik bank')\n",
    "sentence2 = Sentence('Han fik en ny bank')\n",
    "stacked_embeddings.embed(sentence1)\n",
    "stacked_embeddings.embed(sentence2)\n",
    "\n",
    "# Show that it is contextual in the sense that 'bank' has different embedding after context\n",
    "print('{} dimensions out of {} is equal'.format(int(sum(sentence2[4].embedding==sentence1[2].embedding)), len(sentence1[2].embedding)))"
   ]
  },
  {
   "source": [
    "Here is an example of how to use BERT for embedding tokens and sentences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_bert_base_model\n",
    "model = load_bert_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_embedding, sentence_embedding, tokenized_text = model.embed_text('Han sælger frugt')"
   ]
  },
  {
   "source": [
    "### 1.2 Part-of-speech tagging\n",
    "\n",
    "We provide two models for Part-of-speech tagging. Depending on your needs, you might want to use the flair model (better accuracy) or the spaCy model (higher speed). \n",
    "\n",
    "The following snippet shows how to load and use the flair model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_flair_pos_model\n",
    "\n",
    "# Load the POS tagger using the DaNLP wrapper\n",
    "tagger = load_flair_pos_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Using the flair POS tagger\n",
    "sentence = Sentence('Jeg hopper på en bil , som er rød sammen med Niels .') \n",
    "tagger.predict(sentence) \n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "source": [
    "The following snippet shows how to load and use the spaCy model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_spacy_model\n",
    "\n",
    "#Load the POS tagger using the DaNLP wrapper\n",
    "nlp = load_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the spaCy POS tagger\n",
    "doc = nlp('Jeg hopper på en bil, som er rød sammen med Niels.')\n",
    "pred=''\n",
    "for token in doc:\n",
    "    pred += '{} <{}> '.format(token.text, token.pos_)\n",
    "print(pred)"
   ]
  },
  {
   "source": [
    "### 1.3 Named entity recognition\n",
    "\n",
    "We provide 3 models for Named Entity Recognition (NER). \n",
    "\n",
    "Here is an example of how to use the BERT NER model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT NER\n",
    "from danlp.models import load_bert_ner_model\n",
    "bert = load_bert_ner_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of tokens and labels in BIO format\n",
    "tokens, labels = bert.predict(\"Jens Peter Hansen kommer fra Danmark\")\n",
    "print(\" \".join([\"{}/{}\".format(tok,lbl) for tok,lbl in zip(tokens,labels)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a correct tokenization, you have to provide it yourself to BERT  by providing a list of tokens\n",
    "# (for example SpaCy can be used for tokenization)\n",
    "# With this option, output can also be choosen to be a dict with tags and position instead of BIO format\n",
    "tekst_tokenized = ['Han', 'hedder', 'Anders', 'And', 'Andersen', 'og', 'bor', 'i', 'Århus', 'C']\n",
    "bert.predict(tekst_tokenized, IOBformat=False)"
   ]
  },
  {
   "source": [
    "Below is an example for using the flair NER tagger."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_flair_ner_model\n",
    "\n",
    "# Load the NER tagger using the DaNLP wrapper\n",
    "flair_model = load_flair_ner_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Using the flair NER tagger\n",
    "sentence = Sentence('Jens Peter Hansen kommer fra Danmark') \n",
    "flair_model.predict(sentence) \n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "source": [
    "Here is an example for NER with spaCy. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from danlp.models import load_spacy_model\n",
    "\n",
    "nlp = load_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spaCy for NER\n",
    "doc = nlp('Jens Peter Hansen kommer fra Danmark') \n",
    "for tok in doc:\n",
    "    print(\"{} {}\".format(tok,tok.ent_type_))"
   ]
  },
  {
   "source": [
    "### 1.4. Dependency Parsing & Noun Phrase Chunking\n",
    "\n",
    "We provide Dependency parsing with our spaCy model, as well as a wrapper for deducing NP-chunks from dependencies. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from danlp.models import load_spacy_model\n",
    "\n",
    "nlp = load_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the spaCy model for dependency parsing only\n",
    "\n",
    "text = 'Et syntagme er en gruppe af ord, der hænger sammen'\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and/or use our wrapper for deducing NP-chunks\n",
    "from danlp.models import load_spacy_chunking_model\n",
    "\n",
    "# Load the chunker using the DaNLP wrapper\n",
    "chunker = load_spacy_chunking_model(nlp)\n",
    "\n",
    "# Using the chunker to predict BIO tags\n",
    "np_chunks = chunker.predict(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dependency and chunks features for each token\n",
    "\n",
    "syntactic_features=['Id', 'Text', 'Head', 'Dep', 'NP-chunk']\n",
    "head_format =\"\\033[1m{!s:>11}\\033[0m\" * (len(syntactic_features) )\n",
    "row_format =\"{!s:>11}\" * (len(syntactic_features) )\n",
    "\n",
    "print(head_format.format(*syntactic_features))\n",
    "# Printing dependency and chunking features for each token \n",
    "for token, nc in zip(doc, np_chunks):\n",
    "    print(row_format.format(token.i, token.text, token.head.i, token.dep_, nc))"
   ]
  },
  {
   "source": [
    "### 1.5. Sentiment Analysis\n",
    "\n",
    "With the DaNLP package, we provide 2 BERT models for detecting emotions and tone in texts and a spaCy model for predicting the polarity of a sentence. \n",
    "\n",
    "Below is some code for using BERT for detecting emotions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from danlp.models import load_bert_emotion_model\n",
    "classifier = load_bert_emotion_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the classifier\n",
    "print(classifier.predict('bilen er flot'))\n",
    "print(classifier.predict('jeg ejer en rød bil og det er en god bil'))\n",
    "print(classifier.predict('jeg ejer en rød bil men den er gået i stykker'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities and matching classes names\n",
    "proba = classifier.predict_proba('jeg ejer en rød bil men den er gået i stykker', no_emotion=False)[0]\n",
    "classes = classifier._classes()[0]\n",
    "for cl, pb in zip(classes, proba):\n",
    "    print(cl,'\\t', pb)"
   ]
  },
  {
   "source": [
    "Here is an example for using BERT for tone detection."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from danlp.models import load_bert_tone_model\n",
    "classifier = load_bert_tone_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the classifier\n",
    "print(classifier.predict('Analysen viser, at økonomien bliver forfærdelig dårlig'))\n",
    "print(classifier.predict('Jeg tror alligvel, det bliver godt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities and matching classes names\n",
    "proba = classifier.predict_proba('Analysen viser, at økonomien bliver forfærdelig dårlig')[0]\n",
    "classes = classifier._classes()[0]\n",
    "for cl, pb in zip(classes, proba):\n",
    "    print(cl,'\\t', pb)"
   ]
  },
  {
   "source": [
    "Here is how to use spaCy for sentiment analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from danlp.models import load_spacy_model\n",
    "\n",
    "nlp = load_spacy_model(textcat='sentiment', vectorError=True) \n",
    "# if you got an error saying da.vectors not found, try setting vectorError=True as follow:\n",
    "#nlp = load_spacy_model(textcat='sentiment', vectorError=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "# use the model for predicting the polarity of a sentence\n",
    "doc = nlp(\"Vi er glade for spacy!\")\n",
    "max(doc.cats.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "source": [
    "## 2. Datasets\n",
    "\n",
    "    2.1. Danish Dependency Treebank (DaNE)\n",
    "    2.2. Dacoref\n",
    "    2.3. WikiANN\n",
    "    2.4. Sentiment datasets\n",
    "    2.5. Word similarity datasets\n",
    "    2.6. DanNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.1. Danish Dependency Treebank (DaNE)\n",
    "\n",
    "The DaNE dataset contains annotations for PoS-tagging, Named Entity Recognition and Dependency Parsing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import DDT\n",
    "ddt = DDT()\n",
    "\n",
    "spacy_corpus = ddt.load_with_spacy()\n",
    "flair_corpus = ddt.load_with_flair()\n",
    "conllu_format = ddt.load_as_conllu()"
   ]
  },
  {
   "source": [
    "### 2.2. Dacoref\n",
    "\n",
    "Dacoref can be used for training and testing models for coreference resolution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import Dacoref\n",
    "dacoref = Dacoref()\n",
    "# The corpus can be loaded with or without splitting into train, dev and test in a list in that order\n",
    "corpus = dacoref.load_as_conllu(predefined_splits=True) "
   ]
  },
  {
   "source": [
    "### 2.3. WikiANN\n",
    "\n",
    "WikiANN is annotated with named entity tags."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import WikiAnn\n",
    "wikiann = WikiAnn()\n",
    "\n",
    "spacy_corpus = wikiann.load_with_spacy()\n",
    "flair_corpus = wikiann.load_with_flair()"
   ]
  },
  {
   "source": [
    "### 2.4. Sentiment datasets\n",
    "\n",
    "Europarl Sentiment 1 is annotated with polarity scores (from -5 to 5), while Europarl Sentiment 2 is annotated with polarity tags (‘positive’, ‘neutral’, ‘negative’) and analytics (‘subjective’ , ‘objective’). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import EuroparlSentiment1\n",
    "eurosent = EuroparlSentiment1()\n",
    "\n",
    "df = eurosent.load_with_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import EuroparlSentiment2\n",
    "eurosent = EuroparlSentiment2()\n",
    "\n",
    "df = eurosent.load_with_pandas()"
   ]
  },
  {
   "source": [
    "As well as Europarl Sentiment 1, LCC Sentiment is annotated with polarity scores (from -5 to 5)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import LccSentiment\n",
    "lccsent = LccSentiment()\n",
    "\n",
    "df = lccsent.load_with_pandas()"
   ]
  },
  {
   "source": [
    "### 2.5 Word similarity datasets\n",
    "\n",
    "The word similarity datasets contain lists of words annotated with similarity scores (from 1 to 10). They can be used for evaluating word embedings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import DSD\n",
    "\n",
    "dsd = DSD()\n",
    "dsd.load_with_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import WordSim353Da\n",
    "\n",
    "ws353 = WordSim353Da()\n",
    "ws353.load_with_pandas()"
   ]
  },
  {
   "source": [
    "### 2.6 DanNet\n",
    "\n",
    "DanNet is a lexical database such as Wordnet. \n",
    "You can download the database or use our wrapper for finding synonyms and other type of relation between words in Danish. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import DanNet\n",
    "\n",
    "dannet = DanNet()\n",
    "\n",
    "# you can load the databases if you want to look into the databases by yourself\n",
    "words, wordsenses, relations, synsets = dannet.load_with_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use our functions to search for synonyms, hyperonyms, hyponyms and domains \n",
    "\n",
    "word = \"myre\"\n",
    "print(word)\n",
    "print(\"synonyms : \", dannet.synonyms(word))\n",
    "print(\"hypernyms : \", dannet.hypernyms(word))\n",
    "print(\"hyponyms : \", dannet.hyponyms(word))\n",
    "print(\"domains : \", dannet.domains(word))\n",
    "print(\"meanings : \", dannet.meanings(word))\n",
    "\n",
    "# to help you dive into the databases\n",
    "# we also provide the following functions: \n",
    "\n",
    "print(\"part-of-speech : \", dannet.pos(word))\n",
    "print(\"wordnet relations : \", dannet.wordnet_relations(word, eurowordnet=True))\n",
    "print(\"word ids : \", dannet._word_ids(word))\n",
    "print(\"synset ids : \", dannet._synset_ids(word))\n",
    "i = 11034863\n",
    "print(\"word from id =\",i, \":\", dannet._word_from_id(i))\n",
    "i = 3514\n",
    "print(\"synset from id =\", i, \":\", dannet._synset_from_id(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}